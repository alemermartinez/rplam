---
title: "README"
author: "Alejandra Martinez"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output:
  md_document:
    variant: markdown_github
---

The following real data example of the implementation of robust estimators based on B-splines under partially linear additive model is part of a work in progress done in collaboration with Prof. Dr. Graciela Boente.

Let first install package <code>rplam</code>.

```{r installation, results='hide', cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, message=FALSE}
library(devtools)
install_github("alemermartinez/rplam")
library(rplam)
```

We will use the Boston housing data set available in <code>R</code>.
```{r loading data}
library(MASS)
attach(Boston)
str(Boston)
y <- medv
Z <- as.matrix(ptratio)
X <- cbind(rm, log(tax), log(lstat))
```

The degree selected for the spline basis is 3 for the three additive functions.
```{r spline degree}
degree.spline <- 3
```

Classical estimation of a partially linear additive model
```{r plam.cl}
fit.full.cl <- plam.cl(y,Z,X,degree.spline=degree.spline)
```
and the robust proposal
```{r semilla, echo=FALSE}
set.seed(111)
```
```{r plam.rob, warning=FALSE}
fit.rob <- plam.rob(y,Z,X,degree.spline=degree.spline)
```

The number of terms for the approximation of the additive functions selected by the BIC criteria used for the classical approach is
```{r number of terms classical}
fit.full.cl$kj
```
while for the robust proposal that uses the robust BIC criteria is algo
```{r number of terms robust}
fit.rob$kj
```
The estimations obtained for each linear coefficient for both classical and robust approaches are shown it the following Table.
```{r echo=FALSE, results='asis'}
col2 <- round(c(fit.full.cl$coef.const,fit.full.cl$coef.lin),4)
col3 <- round(c(fit.rob$coef.const,fit.rob$coef.lin),4)
tabla.b1 <- cbind(col2,col3)
row.names(tabla.b1) <- c('$\\beta_0$', "$\\beta_1$")
colnames(tabla.b1) <- c("Classical","Robust")
library(knitr)
kable(tabla.b1,caption = "Table 1")
```

The following three plots correspond to the classical (in red and dashed line) and robust (in blue and solid line) fits for the additive functions with their respectively partial residuals.
```{r ggplot1, echo=FALSE,warning=FALSE}
n <- length(y)
DF2 <- as.data.frame(list(
  rbind(X,X),
  c(rep(NA,n),fit.rob$y  - fit.rob$Z%*%fit.rob$coef.lin - fit.rob$coef.const - rowSums(fit.rob$g.matrix[,-1])),
  c(rep(NA,n),fit.rob$y  - fit.rob$Z%*%fit.rob$coef.lin - fit.rob$coef.const - rowSums(fit.rob$g.matrix[,-2])),
  c(rep(NA,n),fit.rob$y  - fit.rob$Z%*%fit.rob$coef.lin - fit.rob$coef.const - rowSums(fit.rob$g.matrix[,-3])),
  rbind(fit.full.cl$g.matrix,fit.rob$g.matrix),
  c(rep("Classical",n),rep("Robust",n))
))
names(DF2) <- c("x1","x2","x3","re.x1", "re.x2","re.x3", "gx1", "gx2","gx3","Fit")
library(ggplot2)
DF2.rojo <- as.data.frame(list(
  rbind(X),
  rbind(fit.full.cl$g.matrix),
  c(rep("Classical",n))
))
names(DF2.rojo) <- c("x1","x2","x3","gx1","gx2","gx3","Fit")
color.fit <- c("red","blue")

ggplot(DF2,aes(x=x1,y=re.x1,color=Fit)) +
  geom_point(alpha=0.5)+
  geom_line(aes(x=x1,y=gx1),size=1.3,alpha=1,linetype=c(rep(2,n),rep(1,n)))+
  scale_color_manual(values = c("#bb0c00","#0052bb"))+
  theme(
    axis.title.y=element_blank()
)

ggplot(DF2,aes(x=x2,y=re.x2,color=Fit)) +
  geom_point(alpha=0.5)+
  geom_line(aes(x=x2,y=gx2),size=1.3,alpha=1,linetype=c(rep(2,n),rep(1,n)))+
  scale_color_manual(values = c("#bb0c00","#0052bb"))+
  theme(
    axis.title.y=element_blank()
  )

ggplot(DF2,aes(x=x3,y=re.x3,color=Fit)) +
  geom_point(alpha=0.5)+
  geom_line(aes(x=x3,y=gx3),size=1.3,alpha=1,linetype=c(rep(2,n),rep(1,n)))+
  scale_color_manual(values = c("#bb0c00","#0052bb"))+
  theme(
    axis.title.y=element_blank()
  )

```

Even though the shape of the estimates corresponding to the second additive component is similar for both estimators, differences are observed in the estimation of the first and third additive component. To identify potential outliers, we use the boxplot of the residuals obtained by the robust fit.
```{r residuals1}
res.rob <- y-fit.rob$prediction
DF3 <- as.data.frame(list(
  1:n,
  y-fit.rob$prediction
))
names(DF3) <- c("Position","Residuals")
ggplot(DF3,aes(y=Residuals))+
  geom_boxplot(fill='#0052bb',alpha=0.5)+
  theme(
    axis.text.x=element_blank()
  )
```
The residuals detected as outliers are:
```{r boxplot}
names(res.rob) <- 1:n
outliers <- as.numeric(names(boxplot(res.rob, plot=FALSE)$out))
outliers
```


Now, we remove these observations from the original data set and re-calculate the classical estimator
```{r re-fit}
y.del <- y[-outliers]
X.del <- X[-outliers,]
Z.del <- as.matrix(Z[-outliers,])

fit.del.cl <- plam.cl(y.del, Z.del, X.del)
```
The same number of terms for the approximations were selected:
```{r kj classical new}
fit.del.cl$kj
```

New estimated linear coefficients with the previous ones are shown in the following table.
```{r echo=FALSE, results='asis'}
col4 <- round(c(fit.del.cl$coef.const,fit.del.cl$coef.lin),4)
tabla.b1.aux <- cbind(tabla.b1,col4)
row.names(tabla.b1.aux) <- c('$\\beta_0$', "$\\beta_1$")
colnames(tabla.b1.aux) <- c("Classical","Robust","Classical on clean data")
kable(tabla.b1.aux,caption = "Table 2")
```

The following plots correspond to the new curves obtained with the classical fit (in red dashed line) using the data without the potential outliers identified by the robust fit together with the curves obtained by the robust fit on the original data set.

```{r ggplot-final, echo=FALSE,warning=FALSE}
DF3point <- as.data.frame(list(
  X[outliers,],
  (fit.rob$y  - fit.rob$Z%*%fit.rob$coef.lin - fit.rob$coef.const - rowSums(fit.rob$g.matrix[,-1]))[outliers,],
  (fit.rob$y  - fit.rob$Z%*%fit.rob$coef.lin - fit.rob$coef.const - rowSums(fit.rob$g.matrix[,-2]))[outliers,],
  (fit.rob$y  - fit.rob$Z%*%fit.rob$coef.lin - fit.rob$coef.const - rowSums(fit.rob$g.matrix[,-3]))[outliers,]
)
)
names(DF3point) <- c("x1.x","x2.x","x3.x","re.x1","re.x2","re.x3")
DF4 <- as.data.frame(list(
  rbind(X.del,X),
  c(fit.del.cl$y  - fit.del.cl$Z%*%fit.del.cl$coef.lin - fit.del.cl$coef.const - rowSums(fit.del.cl$g.matrix[,-1]),fit.rob$y  - fit.rob$Z%*%fit.rob$coef.lin - fit.rob$coef.const - rowSums(fit.rob$g.matrix[,-1])),
  c(fit.del.cl$y  - fit.del.cl$Z%*%fit.del.cl$coef.lin - fit.del.cl$coef.const - rowSums(fit.del.cl$g.matrix[,-2]),fit.rob$y  - fit.rob$Z%*%fit.rob$coef.lin - fit.rob$coef.const - rowSums(fit.rob$g.matrix[,-2])),
  c(fit.del.cl$y  - fit.del.cl$Z%*%fit.del.cl$coef.lin - fit.del.cl$coef.const - rowSums(fit.del.cl$g.matrix[,-3]),fit.rob$y  - fit.rob$Z%*%fit.rob$coef.lin - fit.rob$coef.const - rowSums(fit.rob$g.matrix[,-3])),
  rbind(fit.del.cl$g.matrix,fit.rob$g.matrix),
  c(rep("Classical",n-length(outliers)),rep("Robust",n))
))
names(DF4) <- c("x1","x2","x3","re.x1", "re.x2","re.x3", "gx1","gx2","gx3","Fit")

ggplot(DF4,aes(x=x1,y=re.x1,color=Fit)) +
  geom_point(alpha=0.5)+
  geom_line(aes(x=x1,y=gx1),size=1.3,alpha=1,linetype=c(rep(2,n-length(outliers)),rep(1,n)))+
  scale_color_manual(values = c("#bb0c00","#0052bb"))+
  geom_point(data=DF3point,aes(x=x1.x,y=re.x1),col='#FF33E3',size=2)+
  theme(
    axis.title.y=element_blank()
  )

ggplot(DF4,aes(x=x2,y=re.x2,color=Fit)) +
  geom_point(alpha=0.5)+
  geom_line(aes(x=x2,y=gx2),size=1.3,alpha=1,linetype=c(rep(2,n-length(outliers)),rep(1,n)))+
  scale_color_manual(values = c("#bb0c00","#0052bb"))+
  geom_point(data=DF3point,aes(x=x2.x,y=re.x2),col='#FF33E3',size=2)+
  theme(
    axis.title.y=element_blank()
  )

ggplot(DF4,aes(x=x3,y=re.x3,color=Fit)) +
  geom_point(alpha=0.5)+
  geom_line(aes(x=x3,y=gx3),size=1.3,alpha=1,linetype=c(rep(2,n-length(outliers)),rep(1,n)))+
  scale_color_manual(values = c("#bb0c00","#0052bb"))+
  geom_point(data=DF3point,aes(x=x3.x,y=re.x3),col='#FF33E3',size=2)+
  theme(
    axis.title.y=element_blank()
  )
```

